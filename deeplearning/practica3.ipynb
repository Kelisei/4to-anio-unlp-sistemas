{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8a9b2ac",
   "metadata": {},
   "source": [
    "# Ejercicio 1\n",
    "Dada la funciÃ³n f(x,y) = x / (2x^2 + 3y^2 + 1)  \n",
    "\n",
    "âˆ‚f/âˆ‚x = (-2x^2 + 3y^2 + 1) / (2x^2 + 3y^2 + 1)^2\n",
    "\n",
    "âˆ‚f/âˆ‚y = (-6xy) / (2x^2 + 3y^2 + 1)^2\n",
    "\n",
    "a) Indique el vector gradiente para ğ‘¥ = 1 e ğ‘¦ = âˆ’1  \n",
    "\n",
    "b) Si a partir de (ğ‘¥ = 0.20; ğ‘¦ = âˆ’1) tuviera que elegir una\n",
    "direcciÃ³n en la cual la funciÃ³n crezca, quÃ© acciÃ³n\n",
    "tomarÃ­a?\n",
    "\n",
    "i. Incrementar los valores de x e y\n",
    "\n",
    "ii. Disminuir los valores de x e y\n",
    "\n",
    "iii. Incrementar el valor de x y disminuir el de y\n",
    "\n",
    "iv. Disminuir el valor de x e incrementar el de y\n",
    "\n",
    "c) Una vez elegida la direcciÃ³n en b), el movimiento a realizar serÃ¡ una fracciÃ³n del mÃ³dulo del vector\n",
    "gradiente. Â¿CuÃ¡l serÃ­a la nueva posiciÃ³n si se considera ğ‘ğ‘™ğ‘“ğ‘ = 0.1? Â¿cuÃ¡l es el valor de la funciÃ³n en\n",
    "la nueva ubicaciÃ³n?\n",
    "\n",
    "d) Considere nuevamente las opciones del inciso b) pero ahora debe buscar una direcciÃ³n, a partir de\n",
    "(ğ‘¥ = 0.20; ğ‘¦ = âˆ’1), en la cual la funciÃ³n disminuya Â¿cuÃ¡l serÃ­a su elecciÃ³n?\n",
    "\n",
    "e) Repita lo solicitado en c) para la direcciÃ³n elegida en d) donde el objetivo fue hallar una ubicaciÃ³n\n",
    "donde la funciÃ³n tome un valor menor.\n",
    "\n",
    "f) Utilice la tÃ©cnica del descenso de gradiente para calcular el valor y la ubicaciÃ³n del mÃ­nimo de la\n",
    "funciÃ³n ğ‘“(ğ‘¥, ğ‘¦) que se observa en la figura.\n",
    "\n",
    "g) Idem f) para el mÃ¡ximo de la funciÃ³n ğ‘“(ğ‘¥, ğ‘¦).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f0432a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08b42205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x/(2*x**2 + 3*y**2 + 1)\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "# Definir variables simbÃ³licas\n",
    "x, y = sp.symbols('x y')\n",
    "\n",
    "# Definir la funciÃ³n\n",
    "f = x / (2*x**2 + 3*y**2 + 1)\n",
    "\n",
    "print(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e47733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âˆ‚f/âˆ‚x = -4*x**2/(2*x**2 + 3*y**2 + 1)**2 + 1/(2*x**2 + 3*y**2 + 1)\n",
      "âˆ‚f/âˆ‚y = -6*x*y/(2*x**2 + 3*y**2 + 1)**2\n",
      "Gradiente âˆ‡f = Matrix([[-4*x**2/(2*x**2 + 3*y**2 + 1)**2 + 1/(2*x**2 + 3*y**2 + 1)], [-6*x*y/(2*x**2 + 3*y**2 + 1)**2]])\n",
      "Gradiente en (1,-1) = Matrix([[0.0555555555555556], [0.166666666666667]])\n"
     ]
    }
   ],
   "source": [
    "# Derivadas parciales\n",
    "df_dx = sp.diff(f, x)\n",
    "df_dy = sp.diff(f, y)\n",
    "\n",
    "print(\"âˆ‚f/âˆ‚x =\", df_dx)\n",
    "print(\"âˆ‚f/âˆ‚y =\", df_dy)\n",
    "\n",
    "# Gradiente\n",
    "grad_f = sp.Matrix([df_dx, df_dy])\n",
    "print(\"Gradiente âˆ‡f =\", grad_f)\n",
    "\n",
    "# Evaluar el gradiente en un punto especÃ­fico (x=1, y=-1)\n",
    "grad_at_point = grad_f.subs({x: 1, y: -1})\n",
    "print(\"Gradiente en (1,-1) =\", grad_at_point.evalf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eb5727",
   "metadata": {},
   "source": [
    "### b) Si a partir de (ğ‘¥ = 0.20; ğ‘¦ = âˆ’1) tuviera que elegir una direcciÃ³n en la cual la funciÃ³n crezca, quÃ© acciÃ³n tomarÃ­a?\n",
    "\n",
    "i. Incrementar los valores de x e y  --> Correcta, el gradiente aumenta en x e y\n",
    "\n",
    "ii. Disminuir los valores de x e y\n",
    "\n",
    "iii. Incrementar el valor de x y disminuir el de y\n",
    "\n",
    "iv. Disminuir el valor de x e incrementar el de y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ebdc2813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gradiente en (0.20,-1) = Matrix([[0.235486351403306], [0.0720876585928489]])\n"
     ]
    }
   ],
   "source": [
    "grad_at_point = grad_f.subs({x: 0.20, y: -1})\n",
    "print(\"Gradiente en (0.20,-1) =\", grad_at_point.evalf())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cf6b97",
   "metadata": {},
   "source": [
    "### c) Una vez elegida la direcciÃ³n en b), el movimiento a realizar serÃ¡ una fracciÃ³n del mÃ³dulo del vector gradiente. Â¿CuÃ¡l serÃ­a la nueva posiciÃ³n si se considera ğ‘ğ‘™ğ‘“ğ‘ = 0.1? Â¿cuÃ¡l es el valor de la funciÃ³n en la nueva ubicaciÃ³n?\n",
    "\n",
    "Suponiendo un paso de esta forma:\n",
    "(xnuevo, ynuevo) = (x0, y0) + alfa * âˆ‡f(x0, y0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b729a1de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva posiciÃ³n: (0.223548635140331, -0.992791234140715)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "x0, y0 = 0.20, -1\n",
    "new_position = (x0 + alpha * grad_at_point[0], y0 + alpha * grad_at_point[1])\n",
    "print(\"Nueva posiciÃ³n:\", new_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae8b79b",
   "metadata": {},
   "source": [
    "### d) Considere nuevamente las opciones del inciso b) pero ahora debe buscar una direcciÃ³n, a partir de (ğ‘¥ = 0.20; ğ‘¦ = âˆ’1), en la cual la funciÃ³n disminuya Â¿cuÃ¡l serÃ­a su elecciÃ³n?\n",
    "\n",
    "La idea seria disminuir en x e y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40e7b3a",
   "metadata": {},
   "source": [
    "### e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "891e283e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nueva posiciÃ³n: (0.176451364859669, -1.00720876585928)\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.1\n",
    "x0, y0 = 0.20, -1\n",
    "new_position = (x0 + alpha * -grad_at_point[0], y0 + alpha * -grad_at_point[1])\n",
    "print(\"Nueva posiciÃ³n:\", new_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21470d12",
   "metadata": {},
   "source": [
    "### f) Utilice la tÃ©cnica del descenso de gradiente para calcular el valor y la ubicaciÃ³n del mÃ­nimo de la funciÃ³n ğ‘“(ğ‘¥, ğ‘¦) que se observa en la figura.\n",
    "\n",
    "### g) Idem f) para el mÃ¡ximo de la funciÃ³n ğ‘“(ğ‘¥, ğ‘¦)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5cfd70ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MÃ­nimo en: Matrix([[-0.711096959054583], [-0.00193827784364301]])\n",
      "Valor mÃ­nimo: -0.353545812006922\n",
      "MÃ¡ximo en: Matrix([[0.708187877814801], [-0.000273726833517454]])\n",
      "Valor mÃ¡ximo: 0.353552938327043\n"
     ]
    }
   ],
   "source": [
    "def gradient_descent(starting_point, learning_rate, num_iterations, error_threshold=1e-6):\n",
    "    point = sp.Matrix(starting_point)\n",
    "    for _ in range(num_iterations):\n",
    "        grad = grad_f.subs({x: point[0], y: point[1]}).evalf()\n",
    "        point = point - learning_rate * grad\n",
    "        if grad.norm() < error_threshold:\n",
    "            break\n",
    "    return point, f.subs({x: point[0], y: point[1]}).evalf()\n",
    "\n",
    "# ParÃ¡metros\n",
    "starting_point = (0.20, -1)\n",
    "learning_rate = 0.1\n",
    "num_iterations = 100\n",
    "min_point, min_value = gradient_descent(starting_point, learning_rate, num_iterations)\n",
    "print(\"MÃ­nimo en:\", min_point)\n",
    "print(\"Valor mÃ­nimo:\", min_value)\n",
    "max_point, max_value = gradient_descent(starting_point, -learning_rate, num_iterations)\n",
    "print(\"MÃ¡ximo en:\", max_point)\n",
    "print(\"Valor mÃ¡ximo:\", max_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8824cc26",
   "metadata": {},
   "source": [
    "# Ejercicio 2\n",
    "El precio de una vivienda estÃ¡ dado por dos factores, la proximidad a centros comerciales, financieros, vÃ­as\n",
    "y demÃ¡s que se mide a travÃ©s del factor j, y de la condiciÃ³n misma de la vivienda que se mide por el factor k.\n",
    "La relaciÃ³n entre estos factores y el precio de la vivienda en un barrio determinado estÃ¡ dada por:\n",
    "\n",
    "P(j,k) = k^2 - 2jk + 2j^2 + 18j - 20k + 201\n",
    "\n",
    "donde el precio estÃ¡ dado en miles de dÃ³lares.\n",
    "Utilice la tÃ©cnica de descenso de gradiente para determinar los valores de j y k que minimizan el precio de\n",
    "la vivienda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d60fe1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4*j - 2*k + 18, -2*j + 2*k - 20]\n",
      "MÃ­nimo en: Matrix([[0.999999399436555], [10.9999990282679]])\n",
      "Valor mÃ­nimo: 100.000000000000\n"
     ]
    }
   ],
   "source": [
    "import sympy as sp\n",
    "\n",
    "j, k = sp.symbols('j k')\n",
    "P = k**2 - 2*j*k + 2*j**2 + 18*j - 20*k + 201\n",
    "\n",
    "P_grad = [sp.diff(P, j), sp.diff(P, k)]  # gradiente\n",
    "print(P_grad)\n",
    "\n",
    "def gradient_descent(starting_point, gradient, learning_rate, num_iterations, error_threshold=1e-6):\n",
    "    point = sp.Matrix(starting_point)\n",
    "    for _ in range(num_iterations):\n",
    "        grad = sp.Matrix(gradient).subs({j: point[0], k: point[1]}).evalf()\n",
    "        point = point - learning_rate * grad\n",
    "        if grad.norm() < error_threshold:\n",
    "            break\n",
    "    return point, P.subs({j: point[0], k: point[1]}).evalf()\n",
    "\n",
    "min_point, min_value = gradient_descent((0, 0), P_grad, 0.1, 1000)\n",
    "\n",
    "print(\"MÃ­nimo en:\", min_point)\n",
    "print(\"Valor mÃ­nimo:\", min_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec81d63",
   "metadata": {},
   "source": [
    "# Ejercicio 3\n",
    "A continuaciÃ³n, se presentan datos de entrenamiento y tiempo promedio de un recorrido de corredores\n",
    "para una distancia fija. Los datos estÃ¡n organizados en una tabla con las horas de entrenamiento semanales\n",
    "y el tiempo promedio en minutos para completar el recorrido:\n",
    "\n",
    "| Horas de Entrenamiento | Tiempo Promedio (min) |\n",
    "|------------------------|----------------------|\n",
    "| 2                      | 30.5                 |\n",
    "| 3                      | 29.3                 |\n",
    "| 4                      | 26.7                 |\n",
    "| 4                      | 26.0                 |\n",
    "| 5                      | 25.5                 |\n",
    "| 6                      | 24.7                 |\n",
    "| 6                      | 23.5                 |\n",
    "| 7                      | 23.0                 |\n",
    "| 7                      | 22.5                 |\n",
    "| 8                      | 21.0                 |\n",
    "| 10                     | 21.0                 |\n",
    "| 10                     | 20.5                 |\n",
    "\n",
    "A partir de los datos de entrenamiento y tiempos de recorrido de los corredores, se desea obtener la recta\n",
    "de regresiÃ³n que permita predecir el tiempo promedio que un corredor tardarÃ¡ en recorrer la distancia fija\n",
    "a partir de las horas de entrenamiento semanales. Responder:\n",
    "\n",
    "a) Calcule la correlaciÃ³n lineal entre estas dos variables y dibuje el diagrama de dispersiÃ³n\n",
    "correspondiente. Â¿La recta de regresiÃ³n serÃ¡ de utilidad?\n",
    "\n",
    "b) Indique la ecuaciÃ³n del error cuadrÃ¡tico medio que se deberÃ­a minimizar si se utilizara la tÃ©cnica de\n",
    "descenso de gradiente.\n",
    "\n",
    "c) Indique la ecuaciÃ³n de error que se minimiza al utilizar la tÃ©cnica de descenso de gradiente\n",
    "estocÃ¡stico. Â¿QuÃ© ventaja tiene su uso con respecto a la ecuaciÃ³n indicada en b)?\n",
    "\n",
    "d) Utilice la tÃ©cnica de descenso de gradiente estocÃ¡stico para obtener los coeficientes de la recta de\n",
    "regresiÃ³n que permita predecir el tiempo promedio que un corredor tardarÃ¡ en recorrer la distancia\n",
    "fija a partir de las horas de entrenamiento semanales.\n",
    "\n",
    "e) Utilice la recta obtenida en d) para predecir el tiempo promedio que tardarÃ¡ en recorrer la distancia\n",
    "fija un corredor que entrena 9 horas semanales."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
